# DPO Implementation Plan

This document outlines the strategy for implementing Direct Preference Optimization (DPO) to refine our fine-tuned models and improve ROUGE scores.

## 1. Objective

The primary goal is to align our Supervised Fine-Tuned (SFT) models more closely with the expert "Clinician" responses by training them to prefer these responses over suboptimal alternatives generated by other AI models.

## 2. Implementation Steps

### Step 1: DPO Dataset Preparation

- **Script**: `scripts/prepare_dpo_data.py`
- **Input**: `data/train.csv`
- **Output**: `data/dpo_train_dataset.jsonl`

**Logic**:
1.  Load the `train.csv` file.
2.  For each row, create a JSON object with three keys: `prompt`, `chosen`, and `rejected`.
3.  **`prompt`**: Generate the input prompt using the same logic as in the SFT training (`_create_input_prompt` method).
4.  **`chosen`**: The `Clinician` response. This is the preferred, high-quality example.
5.  **`rejected`**: Select the least optimal response from the `GPT4.0`, `LLAMA`, and `GEMINI` columns. The selection heuristic will be:
    - Prioritize non-null responses.
    - Among available responses, select the one with the shortest character length, as this often indicates a less comprehensive answer.
    - If no AI responses are available for a given case, that case will be skipped.
6.  Save the output as a JSON Lines file, which is an efficient format for this type of data.

### Step 2: DPO Trainer Integration

- **File to Modify**: `core/base_model.py`

**Logic**:
1.  Add a new method, `dpo_fine_tune`, to the `BaseUnslothModel` class.
2.  This method will be responsible for setting up and running the `DPOTrainer` from the `trl` library.
3.  It will take the DPO dataset and DPO-specific hyperparameters (from the config file) as input.
4.  The `DPOTrainer` will be initialized with the SFT-trained model, effectively teaching it to refine its existing knowledge.

### Step 3: DPO Training Script

- **Script**: `scripts/dpo_train.py`

**Logic**:
1.  Create a new main entry point for DPO training.
2.  The script will accept a `--config` argument pointing to a model's YAML configuration file.
3.  It will first load the corresponding SFT-trained model from the `models/` directory.
4.  It will then load the `dpo_train_dataset.jsonl` dataset.
5.  It will call the new `dpo_fine_tune` method on the model instance.
6.  Finally, it will save the DPO-tuned model to a new directory (e.g., `models/Qwen3-0.6B_dpo_finetuned`).

### Step 4: Configuration Update

- **Files to Modify**: `configs/qwen3.yaml`, `configs/llama32.yaml`

**Logic**:
1.  Add a new `dpo_training` section to each configuration file.
2.  This section will contain DPO-specific hyperparameters:
    - `learning_rate`: A much lower learning rate than for SFT (e.g., 5e-7).
    - `beta`: The DPO beta parameter, which controls the strength of the preference signal (e.g., 0.1).
    - `epochs`: DPO typically requires fewer epochs than SFT (e.g., 1-3).
    - `sft_model_path`: A new key to specify the path to the SFT-trained model that will be used as the base for DPO.

## 3. Expected Outcome

The resulting DPO-tuned models should generate responses that are structurally and stylistically more similar to the expert "Clinician" responses, leading to a measurable improvement in our ROUGE scores and a better position on the leaderboard.
