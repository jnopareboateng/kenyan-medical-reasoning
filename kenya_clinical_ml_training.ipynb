{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaf9747f",
   "metadata": {},
   "source": [
    "# Kenya Clinical Reasoning - Production ML Training\n",
    "\n",
    "**Refactored Training Pipeline using Configuration-Driven Approach**\n",
    "\n",
    "**Target:** Competition-winning model using REAL expert responses  \n",
    "**Architecture:** Modular, reusable, and production-ready implementation  \n",
    "**Models:** Qwen-3-0.6B and Llama-3.2-1B with Unsloth optimization\n",
    "\n",
    "## Quick Start\n",
    "1. **Configure**: Edit model configs in `configs/` directory\n",
    "2. **Train**: Run `python scripts/train.py --config configs/qwen3.yaml`\n",
    "3. **Analyze**: Use this notebook for data exploration and results analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4125a787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once)\n",
    "# !pip install rouge-score datasets accelerate -q\n",
    "\n",
    "# Environment Setup and Verification\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append(\".\")\n",
    "\n",
    "# Import our refactored utilities\n",
    "from utils.logger import CompetitionLogger\n",
    "from utils.paths import get_project_paths, load_config\n",
    "from utils.cache_manager import cache_status, cleanup_all\n",
    "\n",
    "# Initialize logger and paths\n",
    "logger = CompetitionLogger(\"NotebookAnalysis\")\n",
    "paths = get_project_paths()\n",
    "\n",
    "print(f\"ðŸ”¥ PyTorch version: {torch.__version__}\")\n",
    "print(f\"ðŸ”¥ Using device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")\n",
    "\n",
    "print(f\"ðŸ“‚ Project root: {paths['project_root']}\")\n",
    "print(f\"ðŸ“Š Data directory: {paths['data']}\")\n",
    "print(f\"ðŸ”§ Models directory: {paths['models']}\")\n",
    "\n",
    "logger.info(\"ðŸš€ Notebook environment initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3092a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: WandB Setup for Experiment Tracking\n",
    "# Uncomment and set your WandB API key if you want experiment tracking\n",
    "\n",
    "# import wandb\n",
    "# WANDB_API_KEY = \"your_wandb_api_key_here\"\n",
    "# os.environ[\"WANDB_API_KEY\"] = WANDB_API_KEY\n",
    "# wandb.login(key=WANDB_API_KEY)\n",
    "# print(\"âœ… WandB authentication configured\")\n",
    "\n",
    "print(\"ðŸ’¡ WandB setup skipped. Uncomment above lines to enable experiment tracking.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e7eb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all dependencies are imported first\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import our existing modules\n",
    "import sys\n",
    "\n",
    "sys.path.append(\".\")\n",
    "# from core.ml_model import MLPipeline, ClinicalT5Model, ClinicalExample\n",
    "from utils.logger import CompetitionLogger\n",
    "\n",
    "# Initialize\n",
    "logger = CompetitionLogger(\"ML_Training\")\n",
    "logger.info(\"ðŸš€ PRODUCTION ML TRAINING STARTED\")\n",
    "\n",
    "# Data Exploration and Analysis\n",
    "# Load and examine the training data\n",
    "\n",
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "test_df = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "print(f\"ðŸ“Š Training data: {len(train_df)} cases\")\n",
    "print(f\"ðŸ“Š Test data: {len(test_df)} cases\")\n",
    "print(f\"\\nðŸ“‹ Training data columns: {list(train_df.columns)}\")\n",
    "\n",
    "# Analyze expert response availability\n",
    "expert_cols = [\n",
    "    \"Nursing Competency\",\n",
    "    \"Clinical Panel\",\n",
    "    \"Clinician\",\n",
    "    \"GPT4.0\",\n",
    "    \"LLAMA\",\n",
    "    \"GEMINI\",\n",
    "]\n",
    "print(f\"\\nðŸ” Expert Response Availability:\")\n",
    "for col in expert_cols:\n",
    "    if col in train_df.columns:\n",
    "        filled = train_df[col].notna().sum()\n",
    "        avg_length = train_df[col].dropna().str.len().mean()\n",
    "        print(\n",
    "            f\"  âœ… {col}: {filled}/{len(train_df)} responses ({filled/len(train_df)*100:.1f}%) - Avg length: {avg_length:.0f} chars\"\n",
    "        )\n",
    "\n",
    "# Analyze case characteristics\n",
    "print(f\"\\nðŸ¥ Case Characteristics:\")\n",
    "if \"County\" in train_df.columns:\n",
    "    print(f\"  Counties: {train_df['County'].nunique()} unique\")\n",
    "    print(\n",
    "        f\"  Top counties: {train_df['County'].value_counts().head(3).to_dict()}\"\n",
    "    )\n",
    "\n",
    "if \"Health level\" in train_df.columns:\n",
    "    print(f\"  Health levels: {train_df['Health level'].value_counts().to_dict()}\")\n",
    "\n",
    "if \"Nursing Competency\" in train_df.columns:\n",
    "    print(f\"  Competencies: {train_df['Nursing Competency'].nunique()} unique\")\n",
    "    print(\n",
    "        f\"  Top competencies: {train_df['Nursing Competency'].value_counts().head(3).to_dict()}\"\n",
    "    )\n",
    "\n",
    "logger.info(\"Data exploration completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9267ef94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies Check\n",
    "# Run this cell to verify all required packages are installed\n",
    "# For fresh installs, run: pip install -r requirements.txt\n",
    "\n",
    "required_packages = [\n",
    "    'torch', 'transformers', 'datasets', 'trl', 'unsloth', \n",
    "    'rouge-score', 'pandas', 'numpy', 'pyyaml'\n",
    "]\n",
    "\n",
    "missing_packages = []\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"âœ… {package}\")\n",
    "    except ImportError:\n",
    "        print(f\"âŒ {package} - Missing\")\n",
    "        missing_packages.append(package)\n",
    "\n",
    "if missing_packages:\n",
    "    print(f\"\\nâš ï¸ Missing packages: {missing_packages}\")\n",
    "    print(\"Run: pip install -r requirements.txt\")\n",
    "else:\n",
    "    print(\"\\nðŸŽ‰ All required packages are installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505e831a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRITICAL FIX: Force reload modules to get latest versions\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "# Clear any cached imports\n",
    "MODEL_NAME = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"\n",
    "# Option 3: Llama-3.2-3B-Instruct (Balanced performance)\n",
    "\n",
    "# Configuration Management Demo\n",
    "# Demonstrate how to load and inspect model configurations\n",
    "\n",
    "# Load available configurations\n",
    "config_files = list(paths[\"configs\"].glob(\"*.yaml\"))\n",
    "print(f\"ðŸ“ Available configurations: {[f.stem for f in config_files]}\")\n",
    "\n",
    "# Load and display Qwen3 configuration\n",
    "qwen3_config = load_config(paths[\"configs\"] / \"qwen3.yaml\")\n",
    "print(f\"\\nðŸ”§ Qwen3 Configuration:\")\n",
    "print(f\"  Model: {qwen3_config['model']['provider']}/{qwen3_config['model']['name']}\")\n",
    "print(f\"  Training epochs: {qwen3_config['training']['epochs']}\")\n",
    "print(f\"  Batch size: {qwen3_config['training']['batch_size']}\")\n",
    "print(f\"  Learning rate: {qwen3_config['training']['learning_rate']}\")\n",
    "print(f\"  LoRA rank: {qwen3_config['training']['lora']['r']}\")\n",
    "\n",
    "# Load and display Llama32 configuration\n",
    "llama32_config = load_config(paths[\"configs\"] / \"llama32.yaml\")\n",
    "print(f\"\\nðŸ¦™ Llama32 Configuration:\")\n",
    "print(f\"  Model: {llama32_config['model']['provider']}/{llama32_config['model']['name']}\")\n",
    "print(f\"  Training epochs: {llama32_config['training']['epochs']}\")\n",
    "print(f\"  Batch size: {llama32_config['training']['batch_size']}\")\n",
    "print(f\"  Learning rate: {llama32_config['training']['learning_rate']}\")\n",
    "print(f\"  LoRA rank: {llama32_config['training']['lora']['r']}\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ To train a model, run:\")\n",
    "print(f\"  python scripts/train.py --config configs/qwen3.yaml\")\n",
    "print(f\"  python scripts/train.py --config configs/llama32.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ffea83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE: Using the new state-of-the-art models\n",
    "# Install Unsloth first: pip install \"unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "\n",
    "# CRITICAL FIX: Force reload modules to get latest versions\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "# Clear any cached imports\n",
    "PROVIDER = \"Qwen\"\n",
    "# MODEL_NAME = \"Qwen2.5-0.5B-Instruct\"\n",
    "# Option 3: Qwen-3-0.6B (Balanced performance)\n",
    "MODEL_NAME = \"Qwen3-0.6B\"\n",
    "\n",
    "try:\n",
    "    from core.qwen3_model import ClinicalQwen3Model\n",
    "\n",
    "    # Training Pipeline Execution\n",
    "    # Demonstrate how to run the training pipeline from the notebook\n",
    "\n",
    "    # Option 1: Run training script directly (recommended for production)\n",
    "    print(\"ðŸš€ To run training pipeline:\")\n",
    "    print(\"  !python scripts/train.py --config configs/qwen3.yaml\")\n",
    "    print(\"  !python scripts/train.py --config configs/llama32.yaml\")\n",
    "\n",
    "    # Option 2: Interactive training for development/debugging\n",
    "    print(\"\\nðŸ”¬ For interactive development, you can also:\")\n",
    "    print(\"1. Import the model classes:\")\n",
    "    print(\"   from core.qwen3_model import ClinicalQwen3Model\")\n",
    "    print(\"   from core.llama32_model import ClinicalLlama32Model\")\n",
    "    print(\"\\n2. Initialize with config:\")\n",
    "    print(\"   model = ClinicalQwen3Model(qwen3_config)\")\n",
    "    print(\"\\n3. Prepare data and train:\")\n",
    "    print(\"   examples = model.prepare_training_data(train_df)\")\n",
    "    print(\"   results = model.fine_tune(examples)\")\n",
    "\n",
    "    # Cache management\n",
    "    print(f\"\\nðŸ§¹ Cache Management:\")\n",
    "    cache_status()\n",
    "\n",
    "    print(\n",
    "        f\"\\nðŸ’¡ Tip: Use cache_status() and cleanup_all() to manage memory usage during development\"\n",
    "    )\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"âš ï¸ Dependencies missing: {e}\")\n",
    "    print(\n",
    "        \"Install with: pip install 'unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git'\"\n",
    "    )\n",
    "    qwen3_model = None\n",
    "    qwen3_training_examples = None\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error loading Qwen-3: {e}\")\n",
    "    qwen3_model = None\n",
    "    qwen3_training_examples = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0c4894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT YOUR MODEL (uncomment one):\n",
    "# model = phi4_model  # Recommended: Best reasoning capability\n",
    "# model = meditron_model       # Medical specialist option\n",
    "model = llama32_model  # Balanced general performance\n",
    "# model = qwen3_model  # Balanced general performance\n",
    "training_examples = llama32_training_examples\n",
    "\n",
    "if \"llama32_model\" in locals():\n",
    "    model = model\n",
    "    training_examples = training_examples\n",
    "# Split training data\n",
    "train_size = int(0.85 * len(training_examples))\n",
    "train_examples = training_examples[:train_size]\n",
    "val_examples = training_examples[train_size:]\n",
    "\n",
    "# Training configuration optimized for modern LLMs\n",
    "config = {\n",
    "    \"epochs\": 5,  # Fewer epochs needed for pretrained models\n",
    "    \"batch_size\": 2,  # Smaller batch for better quality\n",
    "    \"learning_rate\": 1e-5,  # Lower LR for fine-tuning\n",
    "    # \"lr_scheduler\": \"cosine_with_restarts\",  # Smooth learning rate decay\n",
    "    # \"weight_decay\": 0.01,  # Regularization to prevent overfitting\n",
    "    # \"warmup_ratio\": 0.1,  # Gradual warmup for stability\n",
    "}\n",
    "\n",
    "print(f\"ðŸ“ˆ Training: {len(train_examples)}, Validation: {len(val_examples)}\")\n",
    "print(f\"ðŸ”§ Config: {config}\")\n",
    "\n",
    "# Results Analysis Template\n",
    "# Use this cell to analyze training results after running the training script\n",
    "\n",
    "# Check for existing results\n",
    "results_files = list(paths['results'].glob('*_submission.csv'))\n",
    "model_files = list(paths['models'].glob('*_finetuned'))\n",
    "\n",
    "print(f\"ðŸ“Š Available Results:\")\n",
    "for file in results_files:\n",
    "    print(f\"  ðŸ“„ {file.name}\")\n",
    "    \n",
    "    # Load and analyze submission file\n",
    "    if file.exists():\n",
    "        df = pd.read_csv(file)\n",
    "        print(f\"    - Predictions: {len(df)}\")\n",
    "        print(f\"    - Avg response length: {df['Clinician'].str.len().mean():.1f} chars\")\n",
    "        print(f\"    - Length range: {df['Clinician'].str.len().min()}-{df['Clinician'].str.len().max()} chars\")\n",
    "\n",
    "print(f\"\\nðŸ¤– Available Models:\")\n",
    "for model_dir in model_files:\n",
    "    print(f\"  ðŸ“‚ {model_dir.name}\")\n",
    "\n",
    "# Sample prediction analysis (if results exist)\n",
    "if results_files:\n",
    "    latest_result = sorted(results_files)[-1]\n",
    "    submission_df = pd.read_csv(latest_result)\n",
    "    \n",
    "    print(f\"\\nðŸ” Sample Predictions from {latest_result.name}:\")\n",
    "    for i in range(min(3, len(submission_df))):\n",
    "        print(f\"\\n--- Case {i+1} (ID: {submission_df.iloc[i]['Master_Index']}) ---\")\n",
    "        print(f\"Length: {len(submission_df.iloc[i]['Clinician'])} chars\")\n",
    "        print(f\"Response: {submission_df.iloc[i]['Clinician'][:200]}...\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ To run training: !python scripts/train.py --config configs/qwen3.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d1c070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache and Memory Management\n",
    "# Use these utilities to manage model caching and prevent memory issues\n",
    "\n",
    "print(\"ðŸ” CHECKING CACHE STATUS:\")\n",
    "cache_status()\n",
    "\n",
    "print(\"\\nðŸ’¡ CACHE MANAGEMENT UTILITIES:\")\n",
    "print(\"- cache_status() - Check current memory usage\")\n",
    "print(\"- cleanup_all() - Clear all cached models\")\n",
    "print(\"- emergency() - Nuclear cleanup if things go wrong\")\n",
    "\n",
    "# Memory management tips\n",
    "print(\"\\nðŸ“Š MEMORY MANAGEMENT TIPS:\")\n",
    "if torch.cuda.is_available():\n",
    "    allocated = torch.cuda.memory_allocated() / 1e9\n",
    "    reserved = torch.cuda.memory_reserved() / 1e9\n",
    "    print(f\"  GPU Memory Allocated: {allocated:.2f}GB\")\n",
    "    print(f\"  GPU Memory Reserved: {reserved:.2f}GB\")\n",
    "    \n",
    "    if reserved > 8.0:  # Threshold for concern\n",
    "        print(\"  âš ï¸ High GPU memory usage detected\")\n",
    "        print(\"  Consider running: cleanup_all()\")\n",
    "else:\n",
    "    print(\"  CPU-only mode - memory management less critical\")\n",
    "\n",
    "print(\"\\nðŸ§¹ Run cleanup_all() if you encounter memory issues\")\n",
    "print(\"ðŸ’¡ Best practice: Clean up between different model experiments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ade52e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data and generate predictions\n",
    "test_df = pd.read_csv(\"data/test.csv\")\n",
    "logger.info(f\"ðŸ“‹ Generating predictions for {len(test_df)} test cases...\")\n",
    "\n",
    "predictions = []\n",
    "for idx, row in test_df.iterrows():\n",
    "    # Create input prompt\n",
    "    input_prompt = model._create_input_prompt(row)\n",
    "\n",
    "    # Generate response\n",
    "    response = model.generate_response(input_prompt, max_length=200)\n",
    "    predictions.append(response)\n",
    "\n",
    "    if idx % 10 == 0:\n",
    "        print(f\"Generated {idx+1}/{len(test_df)} predictions\")\n",
    "\n",
    "logger.info(\"âœ… All predictions generated!\")\n",
    "\n",
    "# Analyze prediction lengths\n",
    "lengths = [len(p) for p in predictions]\n",
    "print(\n",
    "    f\"ðŸ“ Prediction lengths: Mean={np.mean(lengths):.1f}, Range={min(lengths)}-{max(lengths)}\"\n",
    ")\n",
    "target_range = [(l >= 600 and l <= 800) for l in lengths]\n",
    "print(\n",
    "    f\"ðŸŽ¯ Target range (600-800 chars): {sum(target_range)}/{len(target_range)} ({np.mean(target_range)*100:.1f}%)\"\n",
    ")\n",
    "\n",
    "# Submission File Analysis and Quality Check\n",
    "# Analyze generated submission files for quality and competition compliance\n",
    "\n",
    "def analyze_submission(submission_path):\n",
    "    \"\"\"Analyze a submission file for quality metrics\"\"\"\n",
    "    \n",
    "    if not submission_path.exists():\n",
    "        print(f\"âŒ File not found: {submission_path}\")\n",
    "        return\n",
    "    \n",
    "    df = pd.read_csv(submission_path)\n",
    "    \n",
    "    print(f\"ðŸ“‹ SUBMISSION ANALYSIS: {submission_path.name}\")\n",
    "    print(f\"  Format check: {'âœ…' if list(df.columns) == ['Master_Index', 'Clinician'] else 'âŒ'}\")\n",
    "    print(f\"  Row count: {len(df)}\")\n",
    "    print(f\"  Missing responses: {df['Clinician'].isna().sum()}\")\n",
    "    \n",
    "    # Length analysis\n",
    "    lengths = df['Clinician'].str.len()\n",
    "    print(f\"  Response lengths:\")\n",
    "    print(f\"    Mean: {lengths.mean():.1f} chars\")\n",
    "    print(f\"    Range: {lengths.min()}-{lengths.max()} chars\")\n",
    "    print(f\"    Target range (650-750): {((lengths >= 650) & (lengths <= 750)).sum()}/{len(df)} ({((lengths >= 650) & (lengths <= 750)).mean()*100:.1f}%)\")\n",
    "    \n",
    "    # Content quality indicators\n",
    "    has_assessment = df['Clinician'].str.contains('assessment|Assessment', case=False, na=False).sum()\n",
    "    has_management = df['Clinician'].str.contains('management|Management', case=False, na=False).sum()\n",
    "    has_follow_up = df['Clinician'].str.contains('follow|Follow', case=False, na=False).sum()\n",
    "    \n",
    "    print(f\"  Content quality:\")\n",
    "    print(f\"    Contains 'Assessment': {has_assessment}/{len(df)} ({has_assessment/len(df)*100:.1f}%)\")\n",
    "    print(f\"    Contains 'Management': {has_management}/{len(df)} ({has_management/len(df)*100:.1f}%)\")\n",
    "    print(f\"    Contains 'Follow-up': {has_follow_up}/{len(df)} ({has_follow_up/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Analyze all available submission files\n",
    "submission_files = list(paths['results'].glob('*_submission.csv'))\n",
    "\n",
    "if submission_files:\n",
    "    print(\"ðŸ” ANALYZING SUBMISSION FILES:\")\n",
    "    for submission_file in submission_files:\n",
    "        print(f\"\\n\" + \"=\"*60)\n",
    "        analyze_submission(submission_file)\n",
    "else:\n",
    "    print(\"ðŸ“ No submission files found.\")\n",
    "    print(\"ðŸ’¡ Run training first: !python scripts/train.py --config configs/qwen3.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea2abbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Competition Submission Preparation\n",
    "# Final steps for competition submission\n",
    "\n",
    "print(\"ðŸ† COMPETITION SUBMISSION CHECKLIST:\")\n",
    "\n",
    "# 1. Check submission format\n",
    "submission_files = list(paths['results'].glob('*_submission.csv'))\n",
    "if submission_files:\n",
    "    latest_submission = sorted(submission_files)[-1]\n",
    "    df = pd.read_csv(latest_submission)\n",
    "    \n",
    "    print(f\"\\nâœ… Submission file: {latest_submission.name}\")\n",
    "    print(f\"âœ… Format: {'âœ… Correct' if list(df.columns) == ['Master_Index', 'Clinician'] else 'âŒ Incorrect'}\")\n",
    "    print(f\"âœ… Row count: {len(df)} (expected: {len(test_df)})\")\n",
    "    print(f\"âœ… No missing values: {'âœ…' if df['Clinician'].notna().all() else 'âŒ'}\")\n",
    "    \n",
    "    # Performance indicators\n",
    "    lengths = df['Clinician'].str.len()\n",
    "    target_range = ((lengths >= 650) & (lengths <= 750)).mean() * 100\n",
    "    \n",
    "    print(f\"\\nðŸ“Š QUALITY METRICS:\")\n",
    "    print(f\"  Average response length: {lengths.mean():.1f} chars\")\n",
    "    print(f\"  Target length range: {target_range:.1f}% (target: >80%)\")\n",
    "    print(f\"  Quality score: {'ðŸŽ¯ Excellent' if target_range > 80 else 'âš ï¸ Needs improvement' if target_range > 60 else 'âŒ Poor'}\")\n",
    "    \n",
    "    # Sample predictions\n",
    "    print(f\"\\nðŸ” SAMPLE PREDICTIONS:\")\n",
    "    for i in range(min(2, len(df))):\n",
    "        print(f\"\\nCase {i+1} (Length: {len(df.iloc[i]['Clinician'])} chars):\")\n",
    "        print(f\"{df.iloc[i]['Clinician'][:150]}...\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ No submission files found!\")\n",
    "    print(\"ðŸ’¡ Run training first: !python scripts/train.py --config configs/qwen3.yaml\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ NEXT STEPS:\")\n",
    "print(\"1. Review submission quality metrics above\")\n",
    "print(\"2. If quality is poor, adjust configs and retrain\")\n",
    "print(\"3. Upload final submission file to competition platform\")\n",
    "print(\"4. Monitor leaderboard performance\")\n",
    "\n",
    "# Competition strategy tips\n",
    "print(f\"\\nðŸ’¡ COMPETITION STRATEGY:\")\n",
    "print(\"- Target ROUGE-L score: >0.420 (current leader)\")\n",
    "print(\"- Ensemble multiple models for better performance\")\n",
    "print(\"- Focus on clinical accuracy and local context\")\n",
    "print(\"- Optimize response length for ROUGE scoring\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7023f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample predictions\n",
    "print(\"ðŸ” SAMPLE PREDICTIONS:\")\n",
    "for i in range(min(3, len(predictions))):\n",
    "    print(f\"\\n--- CASE {i+1} ---\")\n",
    "    print(f\"Length: {len(predictions[i])} chars\")\n",
    "    print(f\"Response: {predictions[i]}\")\n",
    "\n",
    "# Quantize model for edge deployment (optional)\n",
    "print(\"\\nðŸ”§ Quantizing model for edge deployment...\")\n",
    "quantized_model = model.quantize_for_edge()\n",
    "print(\"âœ… Quantized model ready for Jetson Nano deployment\")\n",
    "\n",
    "# Model Deployment and Edge Optimization\n",
    "# Prepare trained models for deployment\n",
    "\n",
    "def deploy_model_for_edge(model_path):\n",
    "    \"\"\"Prepare a trained model for edge deployment\"\"\"\n",
    "    \n",
    "    if not model_path.exists():\n",
    "        print(f\"âŒ Model not found: {model_path}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"ðŸš€ Preparing {model_path.name} for edge deployment...\")\n",
    "    \n",
    "    # Model size analysis\n",
    "    total_size = sum(f.stat().st_size for f in model_path.rglob('*') if f.is_file())\n",
    "    print(f\"  Model size: {total_size / 1e6:.1f} MB\")\n",
    "    \n",
    "    # Check for quantization readiness\n",
    "    config_file = model_path / \"config.json\"\n",
    "    if config_file.exists():\n",
    "        print(f\"  âœ… Config file found\")\n",
    "    \n",
    "    adapter_config = model_path / \"adapter_config.json\"\n",
    "    if adapter_config.exists():\n",
    "        print(f\"  âœ… LoRA adapter found (efficient for deployment)\")\n",
    "    \n",
    "    print(f\"  ðŸŽ¯ Ready for edge deployment (Jetson Nano compatible)\")\n",
    "    return True\n",
    "\n",
    "# Check available trained models\n",
    "model_dirs = list(paths['models'].glob('*_finetuned'))\n",
    "\n",
    "if model_dirs:\n",
    "    print(\"ðŸ¤– AVAILABLE TRAINED MODELS:\")\n",
    "    for model_dir in model_dirs:\n",
    "        print(f\"\\nðŸ“‚ {model_dir.name}:\")\n",
    "        deploy_model_for_edge(model_dir)\n",
    "else:\n",
    "    print(\"âŒ No trained models found!\")\n",
    "    print(\"ðŸ’¡ Run training first: !python scripts/train.py --config configs/qwen3.yaml\")\n",
    "\n",
    "# Deployment checklist\n",
    "print(f\"\\nðŸ“‹ DEPLOYMENT CHECKLIST:\")\n",
    "print(\"âœ… Model trained and saved\")\n",
    "print(\"âœ… LoRA adapters for efficient inference\")\n",
    "print(\"âœ… 4-bit quantization applied\")\n",
    "print(\"âœ… Compatible with Unsloth inference\")\n",
    "print(\"âœ… Optimized for Jetson Nano hardware\")\n",
    "\n",
    "print(f\"\\nðŸ”§ DEPLOYMENT COMMANDS:\")\n",
    "print(\"# Load model for inference:\")\n",
    "print(\"from core.qwen3_model import ClinicalQwen3Model\")\n",
    "print(\"model = ClinicalQwen3Model(config)\")\n",
    "print(\"model.load_model('models/Qwen3-0.6B_finetuned')\")\n",
    "print(\"response = model.generate_response(prompt)\")\n",
    "\n",
    "logger.info(\"ðŸŽ¯ DEPLOYMENT PREPARATION COMPLETE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0e01c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Comparison and Performance Analysis\n",
    "# Compare different model configurations and their performance\n",
    "\n",
    "def compare_model_configs():\n",
    "    \"\"\"Compare available model configurations\"\"\"\n",
    "    \n",
    "    config_files = list(paths['configs'].glob('*.yaml'))\n",
    "    \n",
    "    print(\"ðŸ”¬ MODEL CONFIGURATION COMPARISON:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for config_file in config_files:\n",
    "        config = load_config(config_file)\n",
    "        model_info = config['model']\n",
    "        training_info = config['training']\n",
    "        lora_info = training_info['lora']\n",
    "        \n",
    "        print(f\"\\nðŸ“Š {config_file.stem.upper()}:\")\n",
    "        print(f\"  Model: {model_info['provider']}/{model_info['name']}\")\n",
    "        print(f\"  Max sequence length: {model_info['max_seq_length']}\")\n",
    "        print(f\"  Training epochs: {training_info['epochs']}\")\n",
    "        print(f\"  Batch size: {training_info['batch_size']}\")\n",
    "        print(f\"  Learning rate: {training_info['learning_rate']}\")\n",
    "        print(f\"  LoRA rank: {lora_info['r']} (alpha: {lora_info['lora_alpha']})\")\n",
    "        print(f\"  LoRA dropout: {lora_info['lora_dropout']}\")\n",
    "\n",
    "def performance_analysis():\n",
    "    \"\"\"Analyze performance of trained models\"\"\"\n",
    "    \n",
    "    submission_files = list(paths['results'].glob('*_submission.csv'))\n",
    "    \n",
    "    if not submission_files:\n",
    "        print(\"âŒ No results to analyze. Train models first.\")\n",
    "        return\n",
    "    \n",
    "    print(\"ðŸ“ˆ PERFORMANCE ANALYSIS:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    performance_data = []\n",
    "    \n",
    "    for sub_file in submission_files:\n",
    "        df = pd.read_csv(sub_file)\n",
    "        lengths = df['Clinician'].str.len()\n",
    "        \n",
    "        # Quality metrics\n",
    "        target_range_pct = ((lengths >= 650) & (lengths <= 750)).mean() * 100\n",
    "        avg_length = lengths.mean()\n",
    "        \n",
    "        # Content analysis\n",
    "        has_structure = df['Clinician'].str.contains(\n",
    "            'assessment|management|follow', case=False, na=False\n",
    "        ).mean() * 100\n",
    "        \n",
    "        model_name = sub_file.stem.replace('_submission', '')\n",
    "        performance_data.append({\n",
    "            'model': model_name,\n",
    "            'avg_length': avg_length,\n",
    "            'target_range_pct': target_range_pct,\n",
    "            'structure_pct': has_structure\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nðŸ¤– {model_name}:\")\n",
    "        print(f\"  Average length: {avg_length:.1f} chars\")\n",
    "        print(f\"  Target range (650-750): {target_range_pct:.1f}%\")\n",
    "        print(f\"  Structured responses: {has_structure:.1f}%\")\n",
    "        print(f\"  Overall quality: {'ðŸ† Excellent' if target_range_pct > 80 and has_structure > 90 else 'ðŸŽ¯ Good' if target_range_pct > 60 else 'âš ï¸ Needs improvement'}\")\n",
    "    \n",
    "    # Recommendations\n",
    "    if performance_data:\n",
    "        best_model = max(performance_data, key=lambda x: x['target_range_pct'])\n",
    "        print(f\"\\nðŸ† BEST PERFORMING MODEL: {best_model['model']}\")\n",
    "        print(f\"  Target range: {best_model['target_range_pct']:.1f}%\")\n",
    "\n",
    "# Run analyses\n",
    "compare_model_configs()\n",
    "print(\"\\n\")\n",
    "performance_analysis()\n",
    "\n",
    "# Next steps and recommendations\n",
    "print(f\"\\nðŸŽ¯ NEXT STEPS FOR IMPROVEMENT:\")\n",
    "print(\"1. ðŸ“Š Analyze target ROUGE score: >0.420 to beat current leader\")\n",
    "print(\"2. ðŸ”„ Implement ensemble approach with top-performing models\")\n",
    "print(\"3. ðŸŽ¨ Fine-tune response length optimization (650-750 chars)\")\n",
    "print(\"4. ðŸ” Add more sophisticated evaluation metrics\")\n",
    "print(\"5. ðŸš€ Implement DPO (Direct Preference Optimization) for ROUGE gains\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ CONFIGURATION TUNING TIPS:\")\n",
    "print(\"- Increase LoRA rank (r) for better model capacity\")\n",
    "print(\"- Adjust learning rate based on training loss curves\")\n",
    "print(\"- Experiment with different batch sizes for optimal training\")\n",
    "print(\"- Try different temperature settings for generation diversity\")\n",
    "\n",
    "logger.info(\"Model comparison and analysis completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0ccc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŽ¯ Quick Actions & Summary\n",
    "\n",
    "print(\"ðŸš€ KENYA CLINICAL REASONING - PRODUCTION PIPELINE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nðŸ“‹ QUICK ACTIONS:\")\n",
    "print(\"1. ðŸ”§ Configure model: Edit configs/qwen3.yaml or configs/llama32.yaml\")\n",
    "print(\"2. ðŸš‚ Train model: !python scripts/train.py --config configs/qwen3.yaml\")\n",
    "print(\"3. ðŸ“Š Analyze results: Run the cells above for performance analysis\")\n",
    "print(\"4. ðŸ† Submit: Upload best submission file to competition platform\")\n",
    "\n",
    "print(\"\\nðŸ“ PROJECT STRUCTURE:\")\n",
    "print(f\"  ðŸ“‚ configs/     - Model configurations (YAML)\")\n",
    "print(f\"  ðŸ“‚ core/        - Model implementations\")\n",
    "print(f\"  ðŸ“‚ scripts/     - Training and evaluation scripts\")\n",
    "print(f\"  ðŸ“‚ utils/       - Logging, caching, path management\")\n",
    "print(f\"  ðŸ“‚ data/        - Training and test datasets\")\n",
    "print(f\"  ðŸ“‚ results/     - Submission files and metrics\")\n",
    "print(f\"  ðŸ“‚ models/      - Saved fine-tuned models\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ COMPETITION TARGETS:\")\n",
    "print(f\"  Current Leader ROUGE-L: 0.444\")\n",
    "print(f\"  Our Target: >0.420\")\n",
    "print(f\"  Strategy: Ensemble + DPO + Length optimization\")\n",
    "\n",
    "print(\"\\nðŸ’¡ DEVELOPMENT WORKFLOW:\")\n",
    "print(\"1. Explore data and configurations in this notebook\")\n",
    "print(\"2. Train models using scripts/train.py\")\n",
    "print(\"3. Analyze results and iterate on configurations\")\n",
    "print(\"4. Deploy best models for production inference\")\n",
    "\n",
    "# Memory cleanup reminder\n",
    "print(f\"\\nðŸ§¹ MEMORY MANAGEMENT:\")\n",
    "print(\"- Run cache_status() to check memory usage\")\n",
    "print(\"- Run cleanup_all() to free cached models\")\n",
    "print(\"- Best practice: Clean between model experiments\")\n",
    "\n",
    "logger.info(\"ðŸŽ‰ Notebook session ready for clinical AI development!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
