{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaf9747f",
   "metadata": {},
   "source": [
    "# Kenya Clinical Reasoning - PRODUCTION ML TRAINING\n",
    "**FLAN-T5-small Fine-tuning on Expert Clinical Data**\n",
    "\n",
    "**Target:** Competition-winning model using REAL expert responses  \n",
    "**Hardware:** Kaggle P100 GPU acceleration  \n",
    "**Model:** Google FLAN-T5-small (77M params, edge-deployable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4125a787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0mmm00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "bigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m🔥 PyTorch version: 2.6.0+cu124\n",
      "✅ AdamW imported from torch.optim (recommended)\n",
      "🔥 Using device: cuda\n",
      "GPU: Tesla P100-PCIE-16GB\n",
      "Memory: 17.1GB\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies (run once)\n",
    "!pip install rouge-score datasets accelerate -q\n",
    "\n",
    "# Setup\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Check PyTorch and transformers compatibility\n",
    "print(f\"🔥 PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Test AdamW import (fixed in newer versions)\n",
    "try:\n",
    "    from torch.optim import AdamW\n",
    "    print(\"✅ AdamW imported from torch.optim (recommended)\")\n",
    "except ImportError:\n",
    "    try:\n",
    "        from transformers import AdamW\n",
    "        print(\"⚠️ AdamW imported from transformers (deprecated)\")\n",
    "    except ImportError:\n",
    "        print(\"❌ AdamW not found - installing latest transformers\")\n",
    "        !pip install --upgrade transformers torch\n",
    "\n",
    "# Check GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"🔥 Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")\n",
    "else:\n",
    "    print(\"⚠️ No GPU available - training will be slower on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20d6a2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/jnopareboateng/kenyan-medical-reasoning.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "302e6e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Enumerating objects: 16, done.\u001b[K\n",
      "remote: Counting objects: 100% (16/16), done.\u001b[K\n",
      "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
      "remote: Total 10 (delta 7), reused 10 (delta 7), pack-reused 0 (from 0)\u001b[K\n",
      "Unpacking objects: 100% (10/10), 4.59 KiB | 939.00 KiB/s, done.\n",
      "From https://github.com/jnopareboateng/kenyan-medical-reasoning\n",
      " * branch            main       -> FETCH_HEAD\n",
      "   0260873..c027a1f  main       -> origin/main\n",
      "Updating 0260873..c027a1f\n",
      "Unpacking objects: 100% (10/10), 4.59 KiB | 939.00 KiB/s, done.\n",
      "From https://github.com/jnopareboateng/kenyan-medical-reasoning\n",
      " * branch            main       -> FETCH_HEAD\n",
      "   0260873..c027a1f  main       -> origin/main\n",
      "Updating 0260873..c027a1f\n",
      "Fast-forward\n",
      " core/llama32_model.py  |  52 \u001b[32m+++++++++++++++++++++++\u001b[m\u001b[31m-\u001b[m\n",
      " core/meditron_model.py |  52 \u001b[32m+++++++++++++++++++++++\u001b[m\u001b[31m-\u001b[m\n",
      " core/phi4_model.py     |  56 \u001b[32m+++++++++++++++++++++++\u001b[m\u001b[31m--\u001b[m\n",
      " utils/cache_manager.py | 153 \u001b[32m+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\u001b[m\n",
      " 4 files changed, 305 insertions(+), 8 deletions(-)\n",
      " create mode 100644 utils/cache_manager.py\n"
     ]
    }
   ],
   "source": [
    "!git pull origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c55fbd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf kenyan-medical-reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d3a0f46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/working'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5baa5bb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.virtual_documents', 'kenyan-medical-reasoning']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"kenyan-medical-reasoning\"\n",
    "working = \"kaggle/working/\"\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11871a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/kenyan-medical-reasoning\n"
     ]
    }
   ],
   "source": [
    "%cd kenyan-medical-reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16e7eb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO | 🚀 PRODUCTION ML TRAINING STARTED\n",
      "📊 Loaded 400 training cases\n",
      "Columns: ['Master_Index', 'County', 'Health level', 'Years of Experience', 'Prompt', 'Nursing Competency', 'Clinical Panel', 'Clinician', 'GPT4.0', 'LLAMA', 'GEMINI', 'DDX SNOMED']\n",
      "✅ Nursing Competency: 400/400 responses (100.0%)\n",
      "✅ Clinical Panel: 400/400 responses (100.0%)\n",
      "✅ Clinician: 400/400 responses (100.0%)\n",
      "✅ GPT4.0: 400/400 responses (100.0%)\n",
      "✅ LLAMA: 400/400 responses (100.0%)\n",
      "✅ GEMINI: 400/400 responses (100.0%)\n",
      "📊 Loaded 400 training cases\n",
      "Columns: ['Master_Index', 'County', 'Health level', 'Years of Experience', 'Prompt', 'Nursing Competency', 'Clinical Panel', 'Clinician', 'GPT4.0', 'LLAMA', 'GEMINI', 'DDX SNOMED']\n",
      "✅ Nursing Competency: 400/400 responses (100.0%)\n",
      "✅ Clinical Panel: 400/400 responses (100.0%)\n",
      "✅ Clinician: 400/400 responses (100.0%)\n",
      "✅ GPT4.0: 400/400 responses (100.0%)\n",
      "✅ LLAMA: 400/400 responses (100.0%)\n",
      "✅ GEMINI: 400/400 responses (100.0%)\n"
     ]
    }
   ],
   "source": [
    "# Ensure all dependencies are imported first\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import our existing modules\n",
    "import sys\n",
    "\n",
    "sys.path.append(\".\")\n",
    "# from core.ml_model import MLPipeline, ClinicalT5Model, ClinicalExample\n",
    "from utils.logger import CompetitionLogger\n",
    "\n",
    "# Initialize\n",
    "logger = CompetitionLogger(\"ML_Training\")\n",
    "logger.info(\"🚀 PRODUCTION ML TRAINING STARTED\")\n",
    "\n",
    "# Load training data\n",
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "print(f\"📊 Loaded {len(train_df)} training cases\")\n",
    "print(f\"Columns: {list(train_df.columns)}\")\n",
    "\n",
    "# Check expert response columns\n",
    "expert_cols = [\n",
    "    \"Nursing Competency\",\n",
    "    \"Clinical Panel\",\n",
    "    \"Clinician\",\n",
    "    \"GPT4.0\",\n",
    "    \"LLAMA\",\n",
    "    \"GEMINI\",\n",
    "]\n",
    "for col in expert_cols:\n",
    "    if col in train_df.columns:\n",
    "        filled = train_df[col].notna().sum()\n",
    "        print(\n",
    "            f\"✅ {col}: {filled}/{len(train_df)} responses ({filled/len(train_df)*100:.1f}%)\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5440bd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick import test to verify everything works\n",
    "print(\"🔍 Testing imports...\")\n",
    "\n",
    "try:\n",
    "    from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "    from torch.optim import AdamW\n",
    "    print(\"✅ Transformers and PyTorch imports successful\")\n",
    "    \n",
    "    from core.ml_model import ClinicalT5Model\n",
    "    print(\"✅ Custom ML model import successful\")\n",
    "    \n",
    "    print(\"🎯 All imports working - ready for training!\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"❌ Import error: {e}\")\n",
    "    print(\"Installing missing dependencies...\")\n",
    "    !pip install rouge-score datasets accelerate -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bdcf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize FLAN-T5 model WITH CACHING (no more repeated downloads!)\n",
    "from core.ml_model import ClinicalT5Model\n",
    "\n",
    "# Use caching to prevent repeated model downloads\n",
    "model = ClinicalT5Model(\n",
    "    model_name=\"google/flan-t5-small\",  # 77M params, edge-deployable\n",
    "    cache_dir=\"./models\",  # Persistent disk cache\n",
    "    force_download=False   # Use cache if available\n",
    ")\n",
    "\n",
    "logger.info(\n",
    "    f\"Model loaded: {sum(p.numel() for p in model.model.parameters()):,} parameters\"\n",
    ")\n",
    "\n",
    "# Prepare training examples from REAL expert data\n",
    "training_examples = model.prepare_training_data(train_df)\n",
    "logger.info(f\"✅ Prepared {len(training_examples)} training examples\")\n",
    "\n",
    "# Show sample\n",
    "if training_examples:\n",
    "    sample = training_examples[0]\n",
    "    print(\"📋 SAMPLE TRAINING EXAMPLE:\")\n",
    "    print(f\"Input: {sample.input_text[:200]}...\")\n",
    "    print(f\"Target: {sample.target_response[:200]}...\")\n",
    "    print(f\"Length: {len(sample.target_response)} chars\")\n",
    "\n",
    "# 🧹 CACHE MANAGEMENT: Check memory usage\n",
    "from utils.cache_manager import cache_status\n",
    "print(\"\\n💾 CACHE STATUS AFTER MODEL LOADING:\")\n",
    "cache_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be61fce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧹 FLAN-T5 CACHE MANAGEMENT\n",
    "# Use these utilities to manage memory when training multiple models\n",
    "\n",
    "from utils.cache_manager import cleanup_all, cache_status, emergency\n",
    "\n",
    "print(\"🔍 CURRENT CACHE STATUS:\")\n",
    "cache_status()\n",
    "\n",
    "print(\"\\n💡 MEMORY MANAGEMENT OPTIONS:\")\n",
    "print(\"- model.cleanup_model() - Clean up this T5 model\")\n",
    "print(\"- cleanup_all() - Clear all cached models\")\n",
    "print(\"- emergency() - Nuclear cleanup if memory issues\")\n",
    "\n",
    "# Example: Clean up after training\n",
    "# model.cleanup_model()  # Uncomment when done with this model\n",
    "# cleanup_all()          # Uncomment to clear everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664501d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training data\n",
    "train_size = int(0.85 * len(training_examples))\n",
    "train_examples = training_examples[:train_size]\n",
    "val_examples = training_examples[train_size:]\n",
    "\n",
    "logger.info(f\"📈 Training: {len(train_examples)}, Validation: {len(val_examples)}\")\n",
    "\n",
    "# Training configuration for GPU acceleration\n",
    "config = {\n",
    "    \"epochs\": 3,\n",
    "    \"batch_size\": 8,  # Increase for P100\n",
    "    \"learning_rate\": 3e-5,\n",
    "}\n",
    "\n",
    "logger.info(f\"🔧 Training config: {config}\")\n",
    "\n",
    "# Start training (this will take several minutes on P100)\n",
    "print(\"🚀 STARTING FINE-TUNING...\")\n",
    "training_results = model.fine_tune(\n",
    "    train_examples=train_examples, val_examples=val_examples, **config\n",
    ")\n",
    "\n",
    "logger.info(\"✅ Training completed!\")\n",
    "print(\"📊 Training Results:\")\n",
    "for stat in training_results[\"training_stats\"]:\n",
    "    print(\n",
    "        f\"Epoch {stat['epoch']}: Loss={stat['train_loss']:.4f}, ROUGE-L={stat.get('rouge_l', 0):.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d2dd6f",
   "metadata": {},
   "source": [
    "# 🚀 NEW: State-of-the-Art LLM Models Available\n",
    "\n",
    "**STOP USING OUTDATED MODELS!** We now have three cutting-edge implementations:\n",
    "\n",
    "1. **Phi-4-mini-instruct (3.8B)** - Microsoft's latest reasoning model\n",
    "2. **Meditron-7B** - Medical-specialized model trained on clinical data  \n",
    "3. **Llama-3.2-3B-Instruct** - Meta's latest instruction-tuned model\n",
    "\n",
    "All models use **Unsloth optimization** for 2x faster training and 70% less VRAM usage.\n",
    "\n",
    "## Quick Model Comparison\n",
    "- **Phi-4-mini**: Best overall reasoning, MIT license, 128K context\n",
    "- **Meditron-7B**: Medical specialist, trained on PubMed + clinical guidelines\n",
    "- **Llama-3.2**: Solid general performance, good instruction following\n",
    "\n",
    "**Choose based on your priority: General reasoning (Phi-4), Medical knowledge (Meditron), or Balanced performance (Llama-3.2)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9267ef94",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pip3-autoremove\n",
    "!pip install torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu124\n",
    "!pip install unsloth vllm\n",
    "# !pip install --upgrade transformers==4.52.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f1b7f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'kenyan-medical-reasoning/'\n",
      "/kaggle/working/kenyan-medical-reasoning\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.listdir()\n",
    "%cd kenyan-medical-reasoning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2342e67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git pull origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505e831a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 OPTION 1: Microsoft Phi-4-mini-instruct\n",
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-20 06:01:25.706691: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750399285.901466      74 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750399285.957089      74 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 06-20 06:01:45 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 06-20 06:01:45 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 06-20 06:01:45 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 06-20 06:01:45 [__init__.py:239] Automatically detected platform cuda.\n",
      "🦙 OPTION 3: Llama-3.2-3B-Instruct\n",
      "🦙 OPTION 3: Llama-3.2-3B-Instruct\n",
      "INFO | Loading unsloth/Llama-3.2-3B-Instruct with caching optimization\n",
      "INFO | Loading unsloth/Llama-3.2-3B-Instruct with caching optimization\n",
      "INFO | Downloading/Loading from cache: unsloth/Llama-3.2-3B-Instruct\n",
      "INFO | Downloading/Loading from cache: unsloth/Llama-3.2-3B-Instruct\n",
      "==((====))==  Unsloth 2025.6.2: Fast Llama patching. Transformers: 4.51.3. vLLM: 0.8.5.post1.\n",
      "   \\\\   /|    Tesla P100-PCIE-16GB. Num GPUs = 1. Max memory: 15.888 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 6.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "==((====))==  Unsloth 2025.6.2: Fast Llama patching. Transformers: 4.51.3. vLLM: 0.8.5.post1.\n",
      "   \\\\   /|    Tesla P100-PCIE-16GB. Num GPUs = 1. Max memory: 15.888 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 6.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "INFO | ✅ Model cached in memory for future use\n",
      "INFO | ✅ Model cached in memory for future use\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.6.2 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO | Llama-3.2-3B loaded with 1865526272 parameters\n",
      "INFO | Prepared 400 training examples for Llama-3.2\n",
      "✅ Llama-3.2: 400 examples prepared\n",
      "\n",
      "🎯 Choose your weapon and replace the model variable below!\n",
      "Recommended: phi4_model for best competition performance\n"
     ]
    }
   ],
   "source": [
    "# EXAMPLE: Using the new state-of-the-art models\n",
    "# Install Unsloth first: pip install \"unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "\n",
    "# CRITICAL FIX: Force reload modules to get latest versions\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "# Clear any cached imports\n",
    "if \"core.phi4_model\" in sys.modules:\n",
    "    importlib.reload(sys.modules[\"core.phi4_model\"])\n",
    "if \"core.meditron_model\" in sys.modules:\n",
    "    importlib.reload(sys.modules[\"core.meditron_model\"])\n",
    "if \"core.llama32_model\" in sys.modules:\n",
    "    importlib.reload(sys.modules[\"core.llama32_model\"])\n",
    "\n",
    "# Option 1: Phi-4-mini-instruct (Recommended for best reasoning)\n",
    "print(\"🚀 OPTION 1: Microsoft Phi-4-mini-instruct\")\n",
    "from unsloth import FastLanguageModel\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "HF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "\n",
    "# try:\n",
    "#     from core.phi4_model import ClinicalPhi4Model\n",
    "\n",
    "#     # Initialize Phi-4 model with 4-bit quantization and caching\n",
    "#     phi4_model = ClinicalPhi4Model(\n",
    "#         \"microsoft/Phi-4-mini-instruct\",\n",
    "#         load_in_4bit=True,\n",
    "#         cache_dir=\"./models\",  # This should now work!\n",
    "#     )\n",
    "\n",
    "#     # Use same training examples as before\n",
    "#     phi4_training_examples = phi4_model.prepare_training_data(train_df)\n",
    "#     print(f\"✅ Phi-4: {len(phi4_training_examples)} training examples prepared\")\n",
    "\n",
    "#     # Show memory usage\n",
    "#     if torch.cuda.is_available():\n",
    "#         memory_used = torch.cuda.max_memory_allocated() / 1e9\n",
    "#         print(f\"GPU Memory: {memory_used:.1f}GB (4-bit quantized)\")\n",
    "\n",
    "# except ImportError as e:\n",
    "#     print(f\"⚠️ Unsloth not installed: {e}\")\n",
    "#     print(\n",
    "#         \"Run: pip install 'unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git'\"\n",
    "#     )\n",
    "# except Exception as e:\n",
    "#     print(f\"❌ Error loading model: {e}\")\n",
    "#     print(\"Try restarting the kernel if imports are cached\")\n",
    "\n",
    "# print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# Option 2: Meditron-7B (Medical specialist)\n",
    "# print(\"🏥 OPTION 2: Meditron-7B Medical Specialist\")\n",
    "# try:\n",
    "#     from core.meditron_model import ClinicalMeditronModel\n",
    "\n",
    "#     # Initialize medical specialist model with caching\n",
    "#     meditron_model = ClinicalMeditronModel(\n",
    "#         \"epfl-llm/meditron-7b\", load_in_4bit=True, cache_dir=\"./models\"\n",
    "#     )\n",
    "\n",
    "#     # Prepare medical training data\n",
    "#     meditron_training_examples = meditron_model.prepare_training_data(train_df)\n",
    "#     print(f\"✅ Meditron: {len(meditron_training_examples)} medical examples prepared\")\n",
    "\n",
    "# except ImportError as e:\n",
    "#     print(f\"⚠️ Dependencies missing: {e}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"❌ Error loading Meditron: {e}\")\n",
    "\n",
    "# print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# Option 3: Llama-3.2-3B-Instruct (Balanced performance)\n",
    "print(\"🦙 OPTION 3: Llama-3.2-3B-Instruct\")\n",
    "try:\n",
    "    from core.llama32_model import ClinicalLlama32Model\n",
    "\n",
    "    # Initialize Llama model with caching\n",
    "    llama32_model = ClinicalLlama32Model(\n",
    "        \"unsloth/Llama-3.2-3B-Instruct\", load_in_4bit=True, cache_dir=\"./models\"\n",
    "    )\n",
    "\n",
    "    # Prepare training data\n",
    "    llama32_training_examples = llama32_model.prepare_training_data(train_df)\n",
    "    print(f\"✅ Llama-3.2: {len(llama32_training_examples)} examples prepared\")\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"⚠️ Dependencies missing: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading Llama-3.2: {e}\")\n",
    "\n",
    "print(\"\\n🎯 Choose your weapon and replace the model variable below!\")\n",
    "print(\"Recommended: phi4_model for best competition performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0c4894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded model: unsloth/Llama-3.2-3B-Instruct\n",
      "🚀 TRAINING WITH UNSLOTH/LLAMA-3.2-3B-INSTRUCT\n",
      "Training examples: 400\n",
      "📈 Training: 320, Validation: 80\n",
      "🔧 Config: {'epochs': 2, 'batch_size': 4, 'learning_rate': 2e-05}\n",
      "🚀 STARTING STATE-OF-THE-ART FINE-TUNING...\n",
      "⚡ Using Unsloth: 2x faster, 70% less VRAM\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "920d37de81a64c7cb8204e1fe98d3edb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=4):   0%|          | 0/320 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO | Starting Llama-3.2-3B fine-tuning with Unsloth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 320 | Num Epochs = 8 | Total steps = 160\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 24,313,856/3,000,000,000 (0.81% trained)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 🔥 COMPETITION-WINNING TRAINING EXAMPLE\n",
    "# Use this instead of the outdated FLAN-T5 approach\n",
    "\n",
    "# SELECT YOUR MODEL (uncomment one):\n",
    "# model = phi4_model  # Recommended: Best reasoning capability\n",
    "# model = meditron_model       # Medical specialist option\n",
    "model = llama32_model  # Balanced general performance\n",
    "\n",
    "# For demonstration, let's use Phi-4 (replace with your choice)\n",
    "if \"llama32_model\" in locals():\n",
    "    model = model\n",
    "    training_examples = llama32_training_examples\n",
    "    model_name = \"unsloth/llama-3.2-3b-instruct\"\n",
    "    print(f\"✅ Loaded model: {model.model_name}\")\n",
    "\n",
    "    print(f\"🚀 TRAINING WITH {model_name.upper()}\")\n",
    "    print(f\"Training examples: {len(training_examples)}\")\n",
    "\n",
    "    # Split training data\n",
    "    train_size = int(0.80 * len(training_examples))\n",
    "    train_examples = training_examples[:train_size]\n",
    "    val_examples = training_examples[train_size:]\n",
    "\n",
    "    # Training configuration optimized for modern LLMs\n",
    "    config = {\n",
    "        \"epochs\": 2,  # Fewer epochs needed for pretrained models\n",
    "        \"batch_size\": 4,  # Smaller batch for better quality\n",
    "        \"learning_rate\": 2e-5,  # Lower LR for fine-tuning\n",
    "    }\n",
    "\n",
    "    print(f\"📈 Training: {len(train_examples)}, Validation: {len(val_examples)}\")\n",
    "    print(f\"🔧 Config: {config}\")\n",
    "\n",
    "    # START MODERN LLM TRAINING\n",
    "    print(\"🚀 STARTING STATE-OF-THE-ART FINE-TUNING...\")\n",
    "    print(\"⚡ Using Unsloth: 2x faster, 70% less VRAM\")\n",
    "\n",
    "    # Uncomment to actually train:\n",
    "    training_results = model.fine_tune(\n",
    "        train_examples=train_examples, val_examples=val_examples, **config\n",
    "    )\n",
    "\n",
    "    print(\"✅ Ready to train! Uncomment the training code above to start.\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ No modern models loaded. Install Unsloth first!\")\n",
    "    print(\n",
    "        \"Command: pip install 'unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git'\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede91668",
   "metadata": {},
   "source": [
    "# ⚠️ COMPETITIVE REALITY CHECK\n",
    "\n",
    "**You're about to compete with outdated technology while your competitors are using 2025 models.**\n",
    "\n",
    "## Installation Required for Competition Success:\n",
    "\n",
    "```bash\n",
    "# Install Unsloth (adjust for your CUDA version)\n",
    "pip install \"unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "\n",
    "# Additional dependencies\n",
    "pip install trl datasets xformers bitsandbytes\n",
    "```\n",
    "\n",
    "## Model Performance Comparison:\n",
    "- **FLAN-T5-small (77M, 2022)**: Your current model 📉\n",
    "- **Phi-4-mini (3.8B, 2025)**: 10x better reasoning, MIT license 🚀\n",
    "- **Meditron-7B (7B, 2023)**: Medical specialist, clinical training 🏥\n",
    "- **Llama-3.2-3B (3B, 2024)**: Balanced performance, instruction-tuned 🦙\n",
    "\n",
    "**Stop handicapping yourself. Upgrade immediately or lose the competition.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d1c070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧹 CACHE MANAGEMENT - PREVENT MEMORY WASTE\n",
    "# Use these utilities to manage model caching and prevent repeated downloads\n",
    "\n",
    "from utils.cache_manager import ModelCacheManager, cleanup_all, cache_status, emergency\n",
    "\n",
    "print(\"🔍 CHECKING CACHE STATUS:\")\n",
    "cache_status()\n",
    "\n",
    "print(\"\\n💡 CACHE MANAGEMENT UTILITIES:\")\n",
    "print(\"- cleanup_all() - Clear all cached models\")\n",
    "print(\"- cache_status() - Check current memory usage\") \n",
    "print(\"- emergency() - Nuclear cleanup if things go wrong\")\n",
    "print(\"- ModelCacheManager.cleanup_all_models() - Full cleanup\")\n",
    "\n",
    "# Example: Check memory before and after model loading\n",
    "print(\"\\n📊 BEFORE LOADING MODELS:\")\n",
    "cache_info = ModelCacheManager.get_cache_info()\n",
    "print(f\"Cached models: {cache_info['total_cached_models']}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Memory: {cache_info['gpu_memory_allocated']:.2f}GB\")\n",
    "\n",
    "# When you're done experimenting, clean up:\n",
    "# cleanup_all()  # Uncomment to clean up all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ade52e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data and generate predictions\n",
    "test_df = pd.read_csv(\"data/test.csv\")\n",
    "logger.info(f\"📋 Generating predictions for {len(test_df)} test cases...\")\n",
    "\n",
    "predictions = []\n",
    "for idx, row in test_df.iterrows():\n",
    "    # Create input prompt\n",
    "    input_prompt = model._create_input_prompt(row)\n",
    "\n",
    "    # Generate response\n",
    "    response = model.generate_response(input_prompt, max_length=200)\n",
    "    predictions.append(response)\n",
    "\n",
    "    if idx % 10 == 0:\n",
    "        print(f\"Generated {idx+1}/{len(test_df)} predictions\")\n",
    "\n",
    "logger.info(\"✅ All predictions generated!\")\n",
    "\n",
    "# Analyze prediction lengths\n",
    "lengths = [len(p) for p in predictions]\n",
    "print(\n",
    "    f\"📏 Prediction lengths: Mean={np.mean(lengths):.1f}, Range={min(lengths)}-{max(lengths)}\"\n",
    ")\n",
    "target_range = [(l >= 600 and l <= 800) for l in lengths]\n",
    "print(\n",
    "    f\"🎯 Target range (600-800 chars): {sum(target_range)}/{len(target_range)} ({np.mean(target_range)*100:.1f}%)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea2abbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission file\n",
    "submission_df = pd.DataFrame({\"id\": range(len(predictions)), \"response\": predictions})\n",
    "\n",
    "# Save submission\n",
    "submission_path = \"flan_t5_submission.csv\"\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "logger.info(f\"💾 Submission saved: {submission_path}\")\n",
    "\n",
    "# Save model\n",
    "model_path = \"flan_t5_clinical_model\"\n",
    "model.save_model(model_path)\n",
    "logger.info(f\"🤖 Model saved: {model_path}\")\n",
    "\n",
    "# Create final summary\n",
    "summary = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"model\": \"FLAN-T5-small\",\n",
    "    \"parameters\": sum(p.numel() for p in model.model.parameters()),\n",
    "    \"training_examples\": len(train_examples),\n",
    "    \"validation_examples\": len(val_examples),\n",
    "    \"test_predictions\": len(predictions),\n",
    "    \"mean_response_length\": float(np.mean(lengths)),\n",
    "    \"target_range_percentage\": float(np.mean(target_range) * 100),\n",
    "    \"training_results\": training_results,\n",
    "    \"submission_file\": submission_path,\n",
    "    \"model_path\": model_path,\n",
    "}\n",
    "\n",
    "with open(\"training_summary.json\", \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"🏆 PRODUCTION ML TRAINING COMPLETE!\")\n",
    "print(f\"✅ Model: {summary['parameters']:,} parameters\")\n",
    "print(f\"✅ Submission: {submission_path}\")\n",
    "print(f\"✅ Mean length: {summary['mean_response_length']:.1f} chars\")\n",
    "print(f\"✅ Target range: {summary['target_range_percentage']:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7023f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample predictions\n",
    "print(\"🔍 SAMPLE PREDICTIONS:\")\n",
    "for i in range(min(3, len(predictions))):\n",
    "    print(f\"\\n--- CASE {i+1} ---\")\n",
    "    print(f\"Length: {len(predictions[i])} chars\")\n",
    "    print(f\"Response: {predictions[i]}\")\n",
    "\n",
    "# Quantize model for edge deployment (optional)\n",
    "print(\"\\n🔧 Quantizing model for edge deployment...\")\n",
    "quantized_model = model.quantize_for_edge()\n",
    "print(\"✅ Quantized model ready for Jetson Nano deployment\")\n",
    "\n",
    "# Final download instructions\n",
    "print(\"\\n📥 DOWNLOAD FILES:\")\n",
    "print(\"1. flan_t5_submission.csv - Competition submission\")\n",
    "print(\"2. flan_t5_clinical_model/ - Trained model directory\")\n",
    "print(\"3. training_summary.json - Training metrics\")\n",
    "\n",
    "logger.info(\"🎯 READY FOR COMPETITION SUBMISSION!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0e01c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
